---
title: "Extract records of interest from the EBI GWAS Catalog"
output:
  html_notebook: default
  html_document:
    df_print: paged
---

```{r setup, echo=FALSE}
reticulate::use_condaenv('gwas-biocenv')
library(gwascat)
library(readr)
library(dplyr)
library(stringr)

# Unique developer key used when interacting with the NCBI eutils API
Sys.setenv(ncbi_api_key = "d96bb10ffdca4cfb6ad3c963ba2e9f155d09")
```

```{r Get live data, echo=FALSE, message=FALSE}
source('refresh_gwas_dataset.R')
source('functions/select_gwas_cancer.R')
extract_cancer_data(gwas_data)
```

Now we should have a set of records that only include SNPs that are associated with traits related to cancer.

Next, we will interface with the eutils API to collect the publication data (including PDFs where available) for import into EndNote

# Get Publication data from PubMed via API

## Import functions to generate API URL
```{r functions to generate API URLs}
source('functions/ncbi_eutil_functions.R')
```

## Query PubMed via NCBI API to get MEDLINE data for all target publications
```{r GET MEDLINE data for all target PubMed references}
# Get a list of unique PubMed IDs
gwas_ids <- gwas_data %>% 
    distinct(PUBMEDID, .keep_all = TRUE)

# Concatenate all SNP IDs associated with a given PUBMEDID into a single record for that PUBMEDID
gwas_data <- gwas_data %>% 
    dplyr::group_by(PUBMEDID) %>% 
    dplyr::mutate(SnpCnt = paste0("SNP_",row_number())) %>%
    data.table::setDT() %>% 
    data.table::dcast.data.table(
        PUBMEDID ~ SnpCnt,
        value.var = "SNPS",
        fill = NA,
        drop = TRUE
    ) %>% 
    tidyr::unite(SNPS, -PUBMEDID, sep=",") %>% 
    dplyr::mutate(
        SNPS = stringr::str_replace_all(SNPS, "[NA,]+", ", "),
        SNPS = stringr::str_sub(SNPS, 1, nchar(SNPS)-2)
        # SNPS = substr(SNPS, 1, nchar(SNPS)-2)
        ) %>%
    inner_join(
        distinct(gwas_data, PUBMEDID, .keep_all = TRUE), 
        by = "PUBMEDID"
        ) %>% 
    dplyr::select(PUBMEDID, SNPS = SNPS.x, DISEASE.TRAIT)

# Generate URL for API using those unique PUBMEDIDs
fetch_url <- eutil_fetch_url_generator(
    db='pubmed',
    ids=paste(gwas_ids$PUBMEDID, collapse=','),
    rettype='medline',
    api_key=Sys.getenv('ncbi_api_key')
)

# Submit HTTP GET Request, save Response
r <- httr::GET(fetch_url)

# Save raw (hex) HTTP Response stream (Medline formatted reference data) to plain text file
writeBin(httr::content(r, 'raw'), 'medline.txt')
```

```{r Add custom content fields to Medline dataset}
# Read plain Medline text back in
medline_txt <- readLines("medline.txt")

# Loop through each entry and add custom EndNote fields:
    # "Label" (ED), - Notes the GWAS associated DISEASE.TRAIT
    # "Research Note" - Notes that this record was extracted from the GWAS catalog
    # "Target Detail" (CP) - Contains SNP ids

for(id in 1:length(gwas_data$PUBMEDID)){
    medline_txt <- rsed::sed_insert(
        stream = medline_txt,
        after = paste0("PMID- ", gwas_data$PUBMEDID[id]),
        insertion = paste0("ED  - ", gwas_data$DISEASE.TRAIT[id], "\n", 
                           "EP  - Extracted from GWAS Catalog", "\n",
                           "CP  - ", gwas_data$SNPS[id])
    )
}

# Clean up after for loop
rm(id)

# Write updated Medline data to file
writeLines(medline_txt, 'medline.txt')
```

```{r GET PubMedC PDF documents}
base_ftp_url <- "ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/"

# Load Open Access dataset
oa_files <- readr::read_csv("data/oa_file_list.csv")

# Isolate target publications & generate URL for each one
target_files <- oa_files %>%
    filter(PMID %in% gwas_ids$PUBMEDID) %>%
    mutate(File = paste0(base_ftp_url, File),
           as.character(PMID))

if(!file.exists("downloads")){
    dir.create("downloads")
}

# Download PMC archive files
purrr::map2(target_files$File,
            paste0("downloads/", target_files$PMID, ".tar.gz"),
            download.file)

if(!file.exists("downloads/extracts")){
    dir.create("downloads/extracts")
    }

# Extract PDFs from archive
purrr::map(paste0("downloads/", target_files$PMID, ".tar.gz"),
           untar,
           files = "*.pdf$",
           exdir = "downloads/extracts")

# Rename PDFs to PMCID and move to a single folder
for (dir in list.dirs("downloads/extracts", recursive = FALSE)){

    iter = 0

    for (file in list.files(dir)){
        
        filepath <- paste0(dir, "/", file)

        if(iter == 0){
            file.rename(
                filepath,
                paste0("refs/pdfs/", str_extract(dir, "PMC[\\d]+$"), ".pdf")
            )
        } else {
            file.rename(
                filepath,
                paste0("refs/pdfs/", str_extract(dir, "PMC[\\d]+$"), "_", iter, ".pdf")
            )
        }

        iter = iter + 1
    }
}

# Clean up after for loop
rm(dir, file, filepath, iter)

# Delete archive files & extract folders

list.files(
    "downloads",
    recursive = TRUE,
    include.dirs = FALSE, 
    full.names = TRUE
) %>%  
    file.remove()

```
